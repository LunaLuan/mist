{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_for_NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/LunaLuan/mist/blob/master/RNN_for_NLP.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "BMRjahB9LNSD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**1. Utils:**\n",
        "\n",
        "1 class to save dictionary..."
      ]
    },
    {
      "metadata": {
        "id": "w2i2vwsQLR89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self):\n",
        "        self.word_to_index = {}\n",
        "        self.index_to_word = {}\n",
        "        self.word_freq = defaultdict(int)\n",
        "        self.total_words = 0\n",
        "        self.unknown = '<unk>'\n",
        "        self.add_word(self.unknown, count=0)\n",
        "\n",
        "    def add_word(self, word, count=1):\n",
        "        if word not in self.word_to_index:\n",
        "            index = len(self.word_to_index)\n",
        "            self.word_to_index[word] = index\n",
        "            self.index_to_word[index] = word\n",
        "        self.word_freq[word] += count\n",
        "\n",
        "    def construct(self, words):\n",
        "        for word in words:\n",
        "            self.add_word(word)\n",
        "        self.total_words = float(sum(self.word_freq.values()))\n",
        "        print ('{} total words with {} uniques' \\\n",
        "                .format(self.total_words, len(self.word_freq)))\n",
        "\n",
        "    def encode(self, word):\n",
        "        if word not in self.word_to_index:\n",
        "            word = self.unknown\n",
        "        return self.word_to_index[word]\n",
        "\n",
        "    def decode(self, index):\n",
        "        return self.index_to_word[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word_freq)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8h7mXJPjOPtx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2. Data evaluate and analysis: **"
      ]
    },
    {
      "metadata": {
        "id": "cKqj92uzgaaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Import enviroments:\n",
        "# # !pip install Faker\n",
        "# !pip install keras\n",
        "# !pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hIYI1V7sORs3",
        "colab_type": "code",
        "colab": {},
        "outputId": "5e79a5e4-55fd-481d-a6dd-6d7e78338f61"
      },
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "fake = Faker(\"ja_JP\")\n",
        "\n",
        "print (fake.address())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "愛媛県調布市皇居外苑15丁目22番12号 北青山クレスト039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5cZhPAOXUIWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7fe94e12-a63d-478d-89eb-dd229b4536d6"
      },
      "cell_type": "code",
      "source": [
        "vocab = Vocab()\n",
        "\n",
        "for i in range(500):\n",
        "    address = fake.address()\n",
        "#     print address\n",
        "    \n",
        "    for c in address:\n",
        "        vocab.add_word(c)\n",
        "    \n",
        "print (vocab.word_to_index)\n",
        "print (len(vocab))\n",
        "\n",
        "print (address[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<unk>': 0, '山': 1, '梨': 2, '県': 3, '長': 4, '生': 5, '郡': 6, '柄': 7, '町': 8, '台': 9, '場': 10, '6': 11, '丁': 12, '目': 13, '2': 14, '7': 15, '番': 16, '1': 17, '号': 18, ' ': 19, 'ク': 20, 'レ': 21, 'ス': 22, 'ト': 23, '無': 24, '栗': 25, '屋': 26, '8': 27, '熊': 28, '本': 29, '利': 30, '島': 31, '村': 32, '虎': 33, 'ノ': 34, '門': 35, 'ヒ': 36, 'ル': 37, 'ズ': 38, '森': 39, 'タ': 40, 'ワ': 41, 'ー': 42, '3': 43, '4': 44, '丹': 45, '勢': 46, 'ア': 47, 'バ': 48, 'ン': 49, '9': 50, '大': 51, '阪': 52, '府': 53, '網': 54, '白': 55, '里': 56, '市': 57, '竜': 58, '泉': 59, '0': 60, '5': 61, '福': 62, '岡': 63, '足': 64, '立': 65, '区': 66, '戸': 67, 'コ': 68, '太': 69, '田': 70, 'ヶ': 71, '谷': 72, '千': 73, '葉': 74, '代': 75, '上': 76, '広': 77, '方': 78, '京': 79, 'パ': 80, '愛': 81, '知': 82, '夷': 83, '隅': 84, '御': 85, '宿': 86, '高': 87, '輪': 88, '東': 89, '都': 90, '北': 91, '柿': 92, '木': 93, '沢': 94, '媛': 95, '日': 96, '野': 97, '鳥': 98, '越': 99, '滋': 100, '賀': 101, '横': 102, '浜': 103, '西': 104, '睦': 105, '手': 106, '岐': 107, '阜': 108, '調': 109, '布': 110, '橋': 111, 'ポ': 112, '青': 113, '関': 114, '狛': 115, '江': 116, '鶴': 117, '見': 118, '箭': 119, '坪': 120, '月': 121, '取': 122, '国': 123, '束': 124, '富': 125, '津': 126, '秋': 127, '原': 128, '根': 129, '武': 130, '九': 131, '十': 132, '吉': 133, '羽': 134, '百': 135, 'ハ': 136, 'イ': 137, 'ツ': 138, '沖': 139, '縄': 140, '芝': 141, '浦': 142, '井': 143, '世': 144, '金': 145, '新': 146, '潟': 147, '多': 148, '摩': 149, '瑞': 150, '穂': 151, '湯': 152, '塩': 153, '浅': 154, '草': 155, '鹿': 156, '児': 157, '鎌': 158, 'ケ': 159, '入': 160, '松': 161, '石': 162, '葛': 163, '飾': 164, '中': 165, '三': 166, '依': 167, '美': 168, '奈': 169, '良': 170, '蔵': 171, '徳': 172, '南': 173, '折': 174, '所': 175, 'シ': 176, 'ャ': 177, 'ム': 178, '六': 179, '小': 180, '平': 181, '桜': 182, '塚': 183, '岩': 184, '元': 185, '神': 186, '分': 187, '安': 188, '脚': 189, '栃': 190, '川': 191, '下': 192, '宇': 193, '和': 194, '氏': 195, '家': 196, '香': 197, '古': 198, '笠': 199, '土': 200, '兵': 201, '庫': 202, '印': 203, '旛': 204, '酒': 205, '々': 206, '丘': 207, '喜': 208, '箪': 209, '笥': 210, 'テ': 211, 'ィ': 212, '丸': 213, 'の': 214, '内': 215, '重': 216, '来': 217, '麹': 218, '稲': 219, '城': 220, '港': 221, '卯': 222, '埼': 223, '玉': 224, '宮': 225, '保': 226, '明': 227, '茨': 228, '八': 229, '王': 230, '子': 231, '皇': 232, '居': 233, '外': 234, '苑': 235, '瀬': 236, '公': 237, '園': 238, '墨': 239, '段': 240, '吾': 241, '妻': 242, '隼': 243, '赤': 244, '前': 245, '弥': 246, '口': 247, '海': 248, '道': 249, 'い': 250, 'す': 251, 'み': 252, '光': 253, '権': 254, '現': 255, '堂': 256, '堤': 257, '歌': 258, '押': 259, '間': 260, '佐': 261, '久': 262, '留': 263, '米': 264, '崎': 265, '幸': 266, '群': 267, '馬': 268, '勝': 269, 'ど': 270, 'き': 271, '鉢': 272, '杉': 273, '並': 274, '細': 275, '竹': 276, '形': 277, '荒': 278, '池': 279, '之': 280, '端': 281, '宅': 282, '豊': 283, 'Ｊ': 284, 'Ｐ': 285, '筋': 286, '渡': 287, '辺': 288, '花': 289, '畑': 290, '須': 291, '匝': 292, '瑳': 293, '旭': 294, '四': 295, '磯': 296, '呂': 297, '部': 298, '郷': 299, '文': 300, '寺': 301, '筑': 302, '幡': 303, '藤': 304, '静': 305, '一': 306, '雷': 307, '練': 308, '埜': 309, '河': 310, '街': 311, '天': 312, '祠': 313, '丈': 314, '独': 315, '鈷': 316, '鴨': 317, '比': 318, '我': 319, '孫': 320, '檜': 321, '板': 322, '五': 323, '味': 324, '油': 325, '奥': 326, '君': 327, '出': 328, '庄': 329, 'あ': 330, 'る': 331, '猿': 332, '楽': 333, '昭': 334, '二': 335, 'つ': 336, '室': 337, '袖': 338, '清': 339, '房': 340, '総': 341, '品': 342, '蟇': 343, '沼': 344, '轟': 345, '緑': 346, '栄': 347, '今': 348, 'が': 349, '鍛': 350, '冶': 351, '黒': 352, '鋸': 353, '林': 354, '渋': 355, '央': 356, '梅': 357, '鷹': 358}\n",
            "359\n",
            "山\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V2OhvBaKukpt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5853cbfc-eb70-4232-85d7-e09964d4be77"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  Length of sentences\n",
        "\"\"\"\n",
        "\n",
        "import statistics\n",
        "\n",
        "\n",
        "some_address = [fake.address() for i in range(5000)]\n",
        "len_of_sentences = list(map(len, some_address))\n",
        "\n",
        "print(min(len_of_sentences))\n",
        "print(max(len_of_sentences)) \n",
        "print(statistics.mean(len_of_sentences)) \n",
        "print(statistics.median(len_of_sentences)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n",
            "46\n",
            "26.2924\n",
            "28.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ooBTcWVUujvY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3. Data processing:**"
      ]
    },
    {
      "metadata": {
        "id": "JnLjvNtUmJSL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Create tokenizer function:"
      ]
    },
    {
      "metadata": {
        "id": "YZs9cayXfGg7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.vocab = Vocab()\n",
        "  \n",
        "  def add_sentence(self, sentence):\n",
        "    for c in sentence:\n",
        "      self.vocab.add_word(c)\n",
        "      \n",
        "  def text_to_sequence(self, text):\n",
        "    return list(map(self.vocab.encode, text))\n",
        "  \n",
        "  def sequence_to_text(self, indices):\n",
        "    return list(map(self.vocab.decode, indices))\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# For testing\n",
        "# for i in range(5000):\n",
        "#   address = fake.address()\n",
        "#   tokenizer.add_sentence(address)\n",
        "\n",
        "# print (len(tokenizer.vocab))\n",
        "\n",
        "# address = fake.address()\n",
        "# print (address)\n",
        "\n",
        "# sequence = (tokenizer.text_to_sequence(address))\n",
        "# print (sequence)\n",
        "\n",
        "\n",
        "# # print (tokenizer.vocab.word_to_index)\n",
        "# text = (tokenizer.sequence_to_text(sequence))\n",
        "# print (text)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tlweido0mbJR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Add data to tokenizer:"
      ]
    },
    {
      "metadata": {
        "id": "ilcK_L7vmfTO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "62b61a1c-5491-4d5b-88e0-0a919d4a5e3b"
      },
      "cell_type": "code",
      "source": [
        "fake = Faker(\"ja_JP\")\n",
        "for i in range(100000):\n",
        "  address = fake.address()\n",
        "  tokenizer.add_sentence(address)\n",
        "  \n",
        "print (tokenizer.vocab.word_to_index)\n",
        "print (len(tokenizer.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<unk>': 0, '山': 1, '口': 2, '県': 3, '狛': 4, '江': 5, '市': 6, '平': 7, '須': 8, '賀': 9, '2': 10, '丁': 11, '目': 12, '番': 13, '号': 14, ' ': 15, '戸': 16, '塚': 17, '町': 18, 'ア': 19, 'ー': 20, 'バ': 21, 'ン': 22, '4': 23, '0': 24, '3': 25, '長': 26, '野': 27, '八': 28, '丈': 29, '島': 30, '高': 31, '輪': 32, '6': 33, '1': 34, 'シ': 35, 'テ': 36, 'ィ': 37, '北': 38, '上': 39, '9': 40, '大': 41, '阪': 42, '府': 43, '羽': 44, '村': 45, '四': 46, '区': 47, '5': 48, '香': 49, '川': 50, '横': 51, '浜': 52, '磯': 53, '子': 54, '松': 55, '浦': 56, '西': 57, '8': 58, '形': 59, '東': 60, '日': 61, '光': 62, '群': 63, '馬': 64, '墨': 65, '田': 66, '南': 67, '赤': 68, '神': 69, '奈': 70, '調': 71, '布': 72, '芝': 73, '中': 74, '鉢': 75, '石': 76, 'ャ': 77, 'ル': 78, 'ム': 79, '7': 80, '郷': 81, '屋': 82, '福': 83, '井': 84, '世': 85, '谷': 86, '橋': 87, '場': 88, 'パ': 89, 'レ': 90, 'ス': 91, '脚': 92, '折': 93, '梨': 94, '板': 95, '無': 96, '栗': 97, '三': 98, 'コ': 99, 'ポ': 100, '岐': 101, '阜': 102, '袖': 103, 'ケ': 104, '浅': 105, '草': 106, '静': 107, '岡': 108, '文': 109, '京': 110, '鶴': 111, 'ヶ': 112, '丘': 113, 'ク': 114, 'ト': 115, '重': 116, '街': 117, '道': 118, '九': 119, '段': 120, '茨': 121, '城': 122, '青': 123, '梅': 124, '筋': 125, '細': 126, '竹': 127, '緑': 128, '雷': 129, '門': 130, '佐': 131, '富': 132, '津': 133, '天': 134, '今': 135, '生': 136, '郡': 137, '一': 138, '宮': 139, '崎': 140, '秋': 141, '葉': 142, '原': 143, '御': 144, '蔵': 145, '柿': 146, '木': 147, '沢': 148, '栃': 149, '安': 150, '房': 151, '鋸': 152, '白': 153, '金': 154, '台': 155, '熊': 156, '本': 157, '多': 158, '摩': 159, 'ハ': 160, 'イ': 161, 'ツ': 162, '都': 163, '匝': 164, '瑳': 165, '吾': 166, '妻': 167, '外': 168, '国': 169, '間': 170, '下': 171, '宇': 172, '和': 173, '沖': 174, '縄': 175, '睦': 176, '公': 177, '園': 178, '分': 179, '良': 180, '杉': 181, '並': 182, '入': 183, '里': 184, '葛': 185, '飾': 186, '徳': 187, '小': 188, '泉': 189, '歌': 190, '我': 191, '孫': 192, '新': 193, '昭': 194, '千': 195, '港': 196, '渋': 197, '久': 198, '鷹': 199, '勝': 200, 'ど': 201, 'き': 202, '土': 203, '呂': 204, '部': 205, '愛': 206, '知': 207, '幸': 208, '手': 209, '前': 210, 'ノ': 211, '鳥': 212, '越': 213, '総': 214, '丸': 215, 'の': 216, '内': 217, 'Ｊ': 218, 'Ｐ': 219, 'タ': 220, 'ワ': 221, '弥': 222, '六': 223, '滋': 224, '印': 225, '旛': 226, '栄': 227, '媛': 228, '稲': 229, '武': 230, '権': 231, '現': 232, '堂': 233, '花': 234, '岩': 235, '林': 236, '寺': 237, '依': 238, '埼': 239, '玉': 240, '取': 241, '潟': 242, '明': 243, '桜': 244, '保': 245, '品': 246, '氏': 247, '家': 248, '鍛': 249, '冶': 250, '月': 251, '渡': 252, '辺': 253, '夷': 254, '隅': 255, '宿': 256, '海': 257, '喜': 258, '五': 259, '味': 260, '立': 261, '埜': 262, '兵': 263, '庫': 264, '百': 265, '瀬': 266, '美': 267, '鹿': 268, '児': 269, '森': 270, '麹': 271, '河': 272, '池': 273, '之': 274, '端': 275, '広': 276, '関': 277, '隼': 278, '轟': 279, '虎': 280, '太': 281, '吉': 282, '束': 283, '藤': 284, '堤': 285, '黒': 286, '来': 287, '代': 288, '卯': 289, '箭': 290, '坪': 291, '二': 292, 'つ': 293, '室': 294, '比': 295, '見': 296, '独': 297, '鈷': 298, '利': 299, '方': 300, '旭': 301, '豊': 302, '祠': 303, '筑': 304, '十': 305, '根': 306, '所': 307, '央': 308, '鎌': 309, 'が': 310, '押': 311, '留': 312, '米': 313, '猿': 314, '楽': 315, '湯': 316, '酒': 317, '々': 318, '柄': 319, '丹': 320, '勢': 321, '古': 322, '油': 323, '出': 324, '蟇': 325, '沼': 326, '清': 327, '畑': 328, '笠': 329, '箪': 330, '笥': 331, '荒': 332, '奥': 333, '瑞': 334, '穂': 335, '元': 336, 'ヒ': 337, 'ズ': 338, '君': 339, '宅': 340, '皇': 341, '居': 342, '苑': 343, '竜': 344, 'あ': 345, 'る': 346, '幡': 347, '檜': 348, '庄': 349, '網': 350, '塩': 351, '練': 352, '王': 353, '足': 354, 'い': 355, 'す': 356, 'み': 357, '鴨': 358}\n",
            "359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "35VkwPmCnvst",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Create data train, validation and test:"
      ]
    },
    {
      "metadata": {
        "id": "p6kSDjBaxEvv",
        "colab_type": "code",
        "colab": {},
        "outputId": "c72ced69-0a5c-4409-c292-c4c78a63e289"
      },
      "cell_type": "code",
      "source": [
        "raw_data = [fake.address() for i in range(2)]\n",
        "print (raw_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['香川県三鷹市箪笥町23丁目13番18号 太田ヶ谷パレス052', '長野県横浜市緑区湯本塩原26丁目15番11号 松石シティ706']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L47_SbblnzfI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6b8e6571-0c92-4cbd-b27c-e6a58d9ca5d6"
      },
      "cell_type": "code",
      "source": [
        "# train = list(map(tokenizer.text_to_sequence, [fake.address() for i in range(8000)]))\n",
        "# validation = list(map(tokenizer.text_to_sequence, [fake.address() for i in range(1000)]))\n",
        "# test = list(map(tokenizer.text_to_sequence, [fake.address() for i in range(1000)]))\n",
        "data = list(map(tokenizer.text_to_sequence, [fake.address() for i in range(10000)]))\n",
        "print (data[0])\n",
        "# print (train[0: 2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[76, 50, 3, 51, 52, 6, 163, 304, 47, 1, 74, 193, 66, 10, 34, 11, 12, 34, 34, 13, 23, 14]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "asQpVUX9lr6R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# len_of_sentences = list(map(len, data))\n",
        "# print (len_of_sentences)\n",
        "\n",
        "# print (min(len_of_sentences))\n",
        "# print (max(len_of_sentences))\n",
        "# print (len(len_of_sentences))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i8resv1lsiYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Preprocessing:"
      ]
    },
    {
      "metadata": {
        "id": "HwWN7qQCsKRK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a4106eef-385f-4f43-9a7e-1325843a49b9"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded_data = pad_sequences(data, padding='post')\n",
        "\n",
        "print (list(map(len, data[0:10])))\n",
        "print (list(map(len, padded_data[0:10])))\n",
        "\n",
        "print (padded_data[0:2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/system-gru/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[22, 16, 18, 31, 20, 29, 28, 30, 27, 20]\n",
            "[44, 44, 44, 44, 44, 44, 44, 44, 44, 44]\n",
            "[[ 76  50   3  51  52   6 163 304  47   1  74 193  66  10  34  11  12  34\n",
            "   34  13  23  14   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0]\n",
            " [187  30   3 302  30  47 252 253  23  11  12  10  25  13  23  14   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mew-1j0imRhb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- One-hot vector generate:"
      ]
    },
    {
      "metadata": {
        "id": "XrRuzi2hmavW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "bdf93832-1241-4ac6-debb-cd3ddacccbfb"
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "data_input = padded_data\n",
        "data_output = to_categorical(padded_data)\n",
        "\n",
        "print (data_input[0])\n",
        "print (data_output[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 76  50   3  51  52   6 163 304  47   1  74 193  66  10  34  11  12  34\n",
            "  34  13  23  14   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "77oXWloed1bz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**4. Build models:**"
      ]
    },
    {
      "metadata": {
        "id": "nbtyL4JVJXV6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Create model:"
      ]
    },
    {
      "metadata": {
        "id": "cJe3xRwZe5cF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e5d1fe05-4c8e-47a3-ccaf-7ca38b5ca07d"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.core import Dense\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(\n",
        "    input_dim=len(tokenizer.vocab),\n",
        "    output_dim=200\n",
        "))\n",
        "\n",
        "model.add(Bidirectional(\n",
        "    LSTM(512, activation=\"sigmoid\", return_sequences=True)\n",
        "))\n",
        "\n",
        "model.add(Dense(len(tokenizer.vocab)))\n",
        "\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "print (model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 200)         71800     \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, None, 1024)        2920448   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, None, 359)         367975    \n",
            "=================================================================\n",
            "Total params: 3,360,223\n",
            "Trainable params: 3,360,223\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oBIo2-VGJapi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Train model:"
      ]
    },
    {
      "metadata": {
        "id": "i7g2Zzs_JdEH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "638a423c-1785-4374-fbf1-d44763822ac3"
      },
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x=data_input[0:9900],\n",
        "    y=data_output[0:9900],\n",
        "    batch_size=4,\n",
        "    epochs=5\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6F67wgxmzXef",
        "colab_type": "code",
        "colab": {},
        "outputId": "ce600363-4744-482c-a42b-9e4a8c97d54e"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(\n",
        "    x=data_input[9900:],\n",
        "    y=data_output[9900:]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 3s 35ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[nan, 0.4998039293289185]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "metadata": {
        "id": "kdrPuEXvxEwl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}